{
  "dataset_reader": {
    "name": "classification_datasetreader",
    "data_path": "snips",
    "data_types": [
      "train"
    ]
  },
  "dataset": {
    "name": "classification_dataset",
    "seed": 42,
    "field_to_split": "train",
    "split_fields": [
      "train",
      "valid"
    ],
    "split_proportions": [
      0.9,
      0.1
    ]
  },
  "vocabs": {
    "classes_vocab": {
      "name": "default_vocab",
      "inputs": [
        "y"
      ],
      "level": "token",
      "save_path": "vocabs/snips_classes.dict",
      "load_path": "vocabs/snips_classes.dict",
      "train_now": true
    }
  },
  "model": {
    "name": "intent_model",
    "save_path": "intents/intent_cnn_snips",
    "load_path": "intents/intent_cnn_snips",
    "opt": {
      "train_now": true,
      "kernel_sizes_cnn": [
        1,
        2,
        3
      ],
      "filters_cnn": 256,
      "lear_metrics": [
        "binary_accuracy",
        "fmeasure"
      ],
      "confident_threshold": 0.5,
      "optimizer": "Adam",
      "lear_rate": 0.01,
      "lear_rate_decay": 0.1,
      "loss": "binary_crossentropy",
      "text_size": 15,
      "coef_reg_cnn": 1e-4,
      "coef_reg_den": 1e-4,
      "dropout_rate": 0.5,
      "epochs": 1000,
      "dense_size": 100,
      "model_name": "cnn_model"
    },
    "embedder": {
      "name": "fasttext",
      "save_path": "embeddings/reddit_fasttext_model.bin",
      "load_path": "embeddings/reddit_fasttext_model.bin",
      "emb_module": "fasttext",
      "dim": 100
    },
    "tokenizer": {
      "name": "nltk_tokenizer",
      "tokenizer": "wordpunct_tokenize"
    }
  },
  "train": {
    "epochs": 100,
    "batch_size": 64,
    "metrics": [
      "sets_accuracy"
    ],
    "validation_patience": 5,
    "val_every_n_epochs": 1,
    "log_every_n_epochs": 1,
    "show_examples": false
  }
}